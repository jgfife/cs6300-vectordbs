# CS6300 - Vector Database Comparison Assignment

## Overview

In this assignment, you will gain hands-on experience with different vector database technologies by implementing and comparing two distinct modalities:
1. A local file-backed vector database 
2. An API/service-based vector database

This assignment will help you understand the capabilities, tradeoffs, and appropriate use cases for different vector database approaches.

## Learning Outcomes

By completing this assignment, you will be able to:

1. **Critically analyze** quality vs. speed tradeoffs across vector DBs 
2. **Create** working examples for several vector DB modalities and produce reproducible comparisons 
3. **Apply** understanding of vector database capabilities and how to use them

## Assignment Requirements

### Modalities Selection
You must choose exactly one option from each category below:

#### Local File-Backed Options:
- `faiss` persisted indexes
- `annoy` (file-backed)
- `chromadb` (file-backed)
- `nmslib`
- `scann`

#### API/Service Modality Options:
- `milvus` (open-source)
- `qdrant` (Rust-based)
- `weaviate` (schema-based)
- `chroma` (server mode)
- `postgresql+pgvector`
- `pinecone` (managed cloud)
- `supabase` (managed cloud)
- `aws kendra`
- `google vertex ai`
- `redis` (with vector support)

> **Important**: Ensure you use only free-tier options for managed services. The assignment is designed to be completed without incurring financial costs.

### Dataset Options
You will work with one dataset. The following are suggested:

1. **Movie Plots from Wikipedia**
   - Kaggle: [Movie Plots Dataset](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots)
   - ~134,000 movie plot summaries from Wikipedia
   - Fields: Release Year, Title, Origin, Director, Cast, Genre, Wiki URL, Plot
   - Do not use the full database, unless you have a lot of extra time.

2. **20 Newsgroups Text Corpus** 
   - Scikit-learn built-in dataset:
     ```python
     from sklearn.datasets import fetch_20newsgroups
     data = fetch_20newsgroups(subset='all')
     ```
   - ~18,000 newsgroup posts across 20 categories
   - Already widely used in NLP/IR experiments

3. **Recipes Dataset**
   - Kaggle: [Recipe Ingredients and Reviews](https://www.kaggle.com/datasets/kanaryayi/recipe-ingredients-and-reviews)
   - ~12,000 recipes with ingredients + instructions
   - Use the `clean_recipes.csv` file.
   - Careful in parsing, the separator is `;`.

Choose one of these datasets, or select your own dataset, for your implementation.

For your selected dataset, decide the data that will be used for index generation and the meta data that will be stored.

## Implementation Tasks

### Part 1: Local File-Backed Vector Database (2 hours)
Implement ingestion and search functionality for your chosen file-backed vector database:

1. Create an implementation matching your selection of system
2. Implement methods for:
   - Adding items to the index (`add_items`)
   - Searching for nearest neighbors (`search`)
   - Saving and loading indexes from disk
3. Generate sample queries that will be used for lookup. Make the number of queries roughly 500. In class we discussed having these be generated by an alternate LLM using a crafted prompt. I recommend using a `summarization` pipeline. Generating the queries can take a long time, if your GPU doesn't get used. I recommend creating 5 or 10 queries as a test. Depending on how long it takes, create as many as you can, up to 500.
4. Store all of your data entries in the database. (First try 100, then 1000, then the full set. This is a safety measure, in case it takes too long to store all of them. Store as many as is reasonable.)
5. Measure performance metrics:
   - Indexing time (the time to insert all elements into the database.)
   - Disk usage size
   - Query latency (P50, P95, P99) (this is across your full set of queries)
6. Measure quality metrics:
   - Recall@k
   - nDCG@k
   - Try k = 5, unless you have a reason to try something different.
   - Use these quality metrics for the entire query set, using automated relevance checking. Use the similarity test for relevance checking. Pick a threshold to use for true. Note this is different than we mentioned in class. We are not classifying a statement as true or false.
   - Use these quality metrics for about 30 randomly selected queries, using manual relevance checking.

### Part 2: API/Service Vector Database (2 hours)
Implement connector for your chosen service-based vector database:

1. Create an implementation matching your selection of API
2. Implement methods for:
   - Creating an index
   - Adding items to an index (`add_items`)
   - Searching for nearest neighbors (`search`)
3. You will be using the same queries generated previously.
4. Store all of your data entries in the database.
5. Measure performance metrics:
   - Indexing time (the time to insert all elements into the database.)
   - Query latency (P50, P95, P99) (this is across your full set of queries)
6. Measure quality metrics:
   - Recall@k
   - nDCG@k
   - Try k = 5, unless you have a reason to try something different.
   - Use these quality metrics for the entire query set, using automated relevance checking.
   - Use these quality metrics over ~30 randomly selected queries, using manual relevance checking.

### Part 3: Analysis and Comparison (2 hours)
Produce a comparative analysis addressing:

1. **Quality vs. Performance Tradeoffs**: Compare quality and latency between implementations
2. **Operational Complexity**: Discuss setup and maintenance considerations
3. **Cost/Resource Requirements**: Evaluate resource usage for each approach
4. **Use Case Recommendations**: Provide guidance on when to use each modality

## Deliverables

### Code Structure
- Implement your code in a git repo. This could be a new one, or it could be added to an existing one.
- Document setup instructions for your service connector in README
- Create a benchmarking script that demonstrates both implementations

### Written Report (2â€“4 pages)
Your report should include:

1. **Introduction** - Overview of the assignment and dataset used
2. **Methodology** - Implementation details for both modalities
3. **Results** - Performance comparison charts and analysis
4. **Discussion** - Evaluation of tradeoffs, strengths/limitations
5. **Conclusion** - Recommendations for when to use each approach

### Technical Documentation
- `README.md` with clear setup and reproduction instructions
- Clean, well-documented code in your implementations

## Grading Rubric

| Category | Percentage |
|----------|------------|
| Implementation & Reproducibility | 40% |
| Analysis Quality | 30% |
| Experimental Rigor | 20% |
| Communication | 10% |

## Resources and References

- **Sentence Transformers**: https://www.sbert.net/
- **FAISS docs**: https://github.com/facebookresearch/faiss
- **Annoy docs**: https://github.com/spotify/annoy  
- **Chroma docs**: https://docs.trychroma.com/
- **Milvus docs**: https://milvus.io/docs
- **Qdrant docs**: https://qdrant.tech/documentation/
- **Weaviate docs**: https://weaviate.io/developers/weaviate
- **Pinecone docs**: https://www.pinecone.io/docs/
- **Supabase docs**: https://supabase.com/docs
- **PostgreSQL + pgvector**: https://github.com/pgvector/pgvector
- **syntax_demo.py**: Has examples for using the 3 types of operations using HuggingFace's local models.


## Important Notes

- Use open-source embedding models to avoid API costs (e.g., `all-MiniLM-L6-v2`)
- For managed services, only use free-tier options
- Document any challenges or limitations you encounter with either approach
- Ensure all code is well-documented and follows Python best practices